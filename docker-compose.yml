version: '3.7'
# ====================================== AIRFLOW ENVIRONMENT VARIABLES =======================================
x-environment: &airflow_environment
  - AIRFLOW__CORE__EXECUTOR=LocalExecutor
  - AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS=False
  - AIRFLOW__CORE__LOAD_EXAMPLES=False
  - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql://airflow:airflow@postgres:5432/airflow
  - AIRFLOW__CORE__STORE_DAG_CODE=True
  - AIRFLOW__CORE__STORE_SERIALIZED_DAGS=True
  - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
  # custom vars
  - DATA_VOLUME_PATH=/home/user/Документы/Demand_Forecast_Airflow/
  - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}  # Add your AWS key
  - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}  # Add your AWS secret

x-airflow-image: &airflow_image apache/airflow:2.3.0-python3.8
# ====================================== /AIRFLOW ENVIRONMENT VARIABLES ======================================
services:
  postgres: # OK
    image: postgres:12-alpine
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    ports:
      - "5432:5432"

  init: # OK
    build:
      context: src/scheduler/airflow-docker
      args:
        AIRFLOW_BASE_IMAGE: *airflow_image
    image: airflow-docker
    depends_on:
      - postgres
    environment: *airflow_environment
    entrypoint: /bin/bash
    command: -c 'airflow db init && airflow users create --username admin --password admin --firstname Anonymous --lastname Admin --role Admin --email admin@example.org && airflow connections add fs_default --conn-type fs'

  webserver: # OK
    build:
      context: src/scheduler/airflow-docker
      args:
        AIRFLOW_BASE_IMAGE: *airflow_image
    image: airflow-docker

    restart: always
    depends_on:
      - postgres
    ports:
      - "8080:8080"
    volumes:
      - logs:/opt/airflow/logs
    environment: *airflow_environment
    command: webserver

  scheduler: # OK
    build:
      context: src/scheduler/airflow-docker
      args:
        AIRFLOW_BASE_IMAGE: *airflow_image
    image: airflow-docker
    restart: always
    depends_on:
      - postgres
    volumes:
      - logs:/opt/airflow/logs
      - ./dags/:/opt/airflow/dags/
      - ./data/:/opt/airflow/data/
      - /var/run/docker.sock:/var/run/docker.sock
    environment: *airflow_environment
    command: scheduler

  ml_base:
    build:
      context: src/scheduler/ml-base
    image: ml-base
    restart: "no"

  download:
    build:
      context: src/scheduler/download
    image: download
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
    restart: "no"
    volumes:
      - data-raw:/src/scheduler/download/data/raw
    depends_on:
      - ml_base

  preprocess:
    build:
      context: src/scheduler/preprocess
    image: preprocess
    restart: "no"
    volumes:
      - data-processed:/src/scheduler/preprocess/data/processed
    depends_on:
      - ml_base

  split:
    build:
      context: src/scheduler/split
    image: split
    restart: "no"
    volumes:
      - data-processed:/src/scheduler/preprocess/data/processed
    depends_on:
      - ml_base

  train:
    build:
      context: src/scheduler/train
    image: train
    restart: "no"
    volumes:
      - data-predictions:/src/scheduler/train/data/predictions
    depends_on:
      - ml_base

  upload:
    build:
      context: src/scheduler/upload
    image: upload
    restart: "no"
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
    volumes:
      - data-predictions:/src/scheduler/upload/data/predictions
    depends_on:
      - ml_base

volumes:
  logs:
  data-raw:
  data-processed:
  data-predictions:
